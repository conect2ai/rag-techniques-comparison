{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5a2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c322a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc import ImageRefMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bf622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaismedeiros/Documents/Mestrado/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "from docling.datamodel.base_models import ConversionStatus, InputFormat\n",
    "from docling.datamodel.document import ConversionResult\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a94702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f676eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7072684",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_V2 = True\n",
    "USE_LEGACY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bedf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_documents(\n",
    "    conv_results: Iterable[ConversionResult],\n",
    "    output_dir: Path,\n",
    "):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    partial_success_count = 0\n",
    "\n",
    "    for conv_res in conv_results:\n",
    "        if conv_res.status == ConversionStatus.SUCCESS:\n",
    "            success_count += 1\n",
    "            doc_filename = conv_res.input.file.stem\n",
    "\n",
    "            if USE_V2:\n",
    "                conv_res.document.save_as_markdown(\n",
    "                    output_dir / f\"{doc_filename}.md\",\n",
    "                    image_mode=ImageRefMode.PLACEHOLDER,\n",
    "                )\n",
    "\n",
    "                # Export Docling document format to markdown:\n",
    "                with (output_dir / f\"{doc_filename}.md\").open(\"w\") as fp:\n",
    "                    fp.write(conv_res.document.export_to_markdown())\n",
    "                    \n",
    "                _log.info(f\"Saved: {doc_filename}.md\")\n",
    "\n",
    "            if USE_LEGACY:\n",
    "                \n",
    "                # Export Markdown format:\n",
    "                with (output_dir / f\"{doc_filename}.legacy.md\").open(\n",
    "                    \"w\", encoding=\"utf-8\"\n",
    "                ) as fp:\n",
    "                    fp.write(conv_res.legacy_document.export_to_markdown())\n",
    "\n",
    "                _log.info(f\"Saved: {doc_filename}.md\")\n",
    "                \n",
    "        elif conv_res.status == ConversionStatus.PARTIAL_SUCCESS:\n",
    "            _log.info(\n",
    "                f\"Document {conv_res.input.file} was partially converted with the following errors:\"\n",
    "            )\n",
    "            for item in conv_res.errors:\n",
    "                _log.info(f\"\\t{item.error_message}\")\n",
    "            partial_success_count += 1\n",
    "        else:\n",
    "            _log.info(f\"Document {conv_res.input.file} failed to convert.\")\n",
    "            failure_count += 1\n",
    "\n",
    "    _log.info(\n",
    "        f\"Processed {success_count + partial_success_count + failure_count} docs, \"\n",
    "        f\"of which {failure_count} failed \"\n",
    "        f\"and {partial_success_count} were partially converted.\"\n",
    "    )\n",
    "    return success_count, partial_success_count, failure_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7518d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_paths = [\n",
    "        Path(\"./manuals/Volkswagen_Polo_2025.pdf\"),\n",
    "        Path(\"./manuals/Fiat_Argo_2023.pdf\")\n",
    "    ]\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.generate_page_images = False\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options, backend=DoclingParseV4DocumentBackend\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_results = doc_converter.convert_all(\n",
    "        input_doc_paths,\n",
    "        raises_on_error=False,  # to let conversion run through all and examine results at the end\n",
    "    )\n",
    "    success_count, partial_success_count, failure_count = export_documents(\n",
    "        conv_results, output_dir=Path(\"markdown_manuals\")\n",
    "    )\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Document conversion complete in {end_time:.2f} seconds.\")\n",
    "\n",
    "    if failure_count > 0:\n",
    "        raise RuntimeError(\n",
    "            f\"The example failed converting {failure_count} on {len(input_doc_paths)}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad75d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 70041f74270850b7bedf7c8f5c2dcede\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Volkswagen_Polo_2025.pdf\n",
      "/Users/thaismedeiros/Documents/Mestrado/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:docling.document_converter:Finished converting document Volkswagen_Polo_2025.pdf in 312.05 sec.\n",
      "INFO:__main__:Saved: Volkswagen_Polo_2025.md\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Fiat_Argo_2023.pdf\n",
      "INFO:docling.document_converter:Finished converting document Fiat_Argo_2023.pdf in 190.62 sec.\n",
      "INFO:__main__:Saved: Fiat_Argo_2023.md\n",
      "INFO:__main__:Processed 2 docs, of which 0 failed and 0 were partially converted.\n",
      "INFO:__main__:Document conversion complete in 503.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c2e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_paths = [\n",
    "    \"./markdown_manuals/Volkswagen_Polo_2025.md\",\n",
    "    \"./markdown_manuals/Fiat_Argo_2023.md\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a746914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:--- Processando arquivo: ./markdown_manuals/Volkswagen_Polo_2025.md ---\n",
      "INFO:__main__:Metadados extraídos: Marca=Volkswagen, Modelo=Polo, Ano=2025\n",
      "INFO:__main__:Arquivo processado. 1301 chunks foram criados e adicionados.\n",
      "INFO:__main__:--- Processando arquivo: ./markdown_manuals/Fiat_Argo_2023.md ---\n",
      "INFO:__main__:Metadados extraídos: Marca=Fiat, Modelo=Argo, Ano=2023\n",
      "INFO:__main__:Arquivo processado. 644 chunks foram criados e adicionados.\n",
      "INFO:__main__:Total de 1945 chunks de todos os arquivos prontos para ingestão.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Todos os documentos foram ingeridos no Milvus com sucesso!\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "\n",
    "for markdown_path in markdown_paths:\n",
    "    _log.info(f\"--- Processando arquivo: {markdown_path} ---\")\n",
    "\n",
    "    if not os.path.exists(markdown_path):\n",
    "        _log.warning(f\"Arquivo não encontrado, pulando: {markdown_path}\")\n",
    "        continue\n",
    "\n",
    "    filename = Path(markdown_path).stem\n",
    "    try:\n",
    "        brand, model, year = filename.split('_')\n",
    "        _log.info(f\"Metadados extraídos: Marca={brand}, Modelo={model}, Ano={year}\")\n",
    "    except ValueError:\n",
    "        _log.error(f\"O nome do arquivo '{filename}.md' não segue o padrão 'marca_modelo_ano'. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    loader = UnstructuredMarkdownLoader(markdown_path, mode=\"single\")\n",
    "    docs   = loader.load()\n",
    "\n",
    "    header_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")],\n",
    "        strip_headers=False\n",
    "    )\n",
    "    header_chunks = header_splitter.split_text(docs[0].page_content)\n",
    "\n",
    "    char_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size    = 800,\n",
    "        chunk_overlap = 100\n",
    "    )\n",
    "\n",
    "    final_chunks_for_file = []\n",
    "    for chunk in header_chunks:\n",
    "        chunk.metadata['brand'] = brand\n",
    "        chunk.metadata['model'] = model\n",
    "        chunk.metadata['year'] = year\n",
    "        chunk.metadata['source'] = filename\n",
    "\n",
    "        if len(chunk.page_content) > 1000:\n",
    "            final_chunks_for_file.extend(char_splitter.split_documents([chunk]))\n",
    "        else:\n",
    "            final_chunks_for_file.append(chunk)\n",
    "    \n",
    "    all_documents.extend(final_chunks_for_file)\n",
    "    _log.info(f\"Arquivo processado. {len(final_chunks_for_file)} chunks foram criados e adicionados.\")\n",
    "\n",
    "if not all_documents:\n",
    "    _log.warning(\"Nenhum documento para ingerir. Finalizando o script.\")\n",
    "else:\n",
    "    _log.info(f\"Total de {len(all_documents)} chunks de todos os arquivos prontos para ingestão.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    MILVUS_HOST = \"localhost\"\n",
    "    MILVUS_PORT = \"19530\"\n",
    "    MILVUS_COLLECTION_NAME = \"manuals\"\n",
    "\n",
    "    milvus_uri = f\"http://{MILVUS_HOST}:{MILVUS_PORT}\"\n",
    "    connection_args = {\"uri\": milvus_uri}\n",
    "\n",
    "    vectorstore = Milvus.from_documents(\n",
    "        documents=all_documents,\n",
    "        collection_name=MILVUS_COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        connection_args=connection_args,\n",
    "        auto_id=True,\n",
    "        consistency_level=\"Strong\",\n",
    "        search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    )\n",
    "\n",
    "    _log.info(\"Todos os documentos foram ingeridos no Milvus com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c08606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Milvus(\n",
    "    collection_name=\"manuals\",\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "    auto_id=True,\n",
    "    consistency_level=\"Strong\",\n",
    "    search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
