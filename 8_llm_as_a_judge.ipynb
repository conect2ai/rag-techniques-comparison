{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Avalia respostas de sistemas de RAG, gerando objetos distintos para avaliações\n",
    "individuais e para comparações de similaridade (BERTScore) no mesmo arquivo JSON.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Union\n",
    "from itertools import combinations\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "import textstat\n",
    "\n",
    "from bert_score import score as bert_score_calculate\n",
    "from statistics import mean\n",
    "\n",
    "textstat.set_lang('pt_BR')\n",
    "\n",
    "# --- Configurações e Constantes ---\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler(\"avaliacao_rag.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "MODELO_JUIZ = \"gpt-4o\"\n",
    "TEMPERATURA_JUIZ = 0.0\n",
    "BERTSCORE_MODEL_TYPE = None \n",
    "BERTSCORE_LANG = \"pt\"\n",
    "\n",
    "PROMPT_SISTEMA_JUIZ = \"\"\"Você é um avaliador especialista em sistemas de Resposta a Perguntas (QA)\n",
    "baseados em manuais técnicos automotivos. Sua tarefa é avaliar a qualidade das respostas geradas\n",
    "com base em um critério específico e em uma rubrica ou heurística de pontuação.\n",
    "Seja objetivo, rigoroso e baseie sua justificativa em evidências.\n",
    "Responda SEMPRE no formato JSON solicitado, com as chaves \"pontuacao\" e \"justificativa\".\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- PESOS PADRÃO ----------------\n",
    "PESOS_DEFAULT: Dict[str, float] = {\n",
    "    \"fidelidade\": 0.40,\n",
    "    \"verificacao_seguranca\": 0.30,\n",
    "    \"completude\": 0.20,\n",
    "    \"relevancia\": 0.10,\n",
    "}\n",
    "\n",
    "NOMES_DOS_ARQUIVOS_DE_DADOS = [\n",
    "    \"resultados_rag_fiat.json\",\n",
    "    \"resultados_rag_vw.json\",\n",
    "    \"resultados_rag_gd_fiat.json\",\n",
    "    \"resultados_rag_gd_vw.json\",\n",
    "    \"resultados_rag_multiquery_fiat.json\",\n",
    "    \"resultados_rag_multiquery_vw.json\",\n",
    "    \"resultados_rag_stepback_fiat.json\",\n",
    "    \"resultados_rag_stepback_vw.json\",\n",
    "    \"resultados_selfrag_fiat.json\",\n",
    "    \"resultados_selfrag_vw.json\",\n",
    "    \"resultados_selfragGD_fiat.json\",\n",
    "    \"resultados_selfragGD_vw.json\",\n",
    "]\n",
    "\n",
    "PROMPTS_AVALIACAO = {\n",
    "\n",
    "    # 1. FIDELIDADE ----------------------------------------------------------\n",
    "    \"fidelidade\": ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(PROMPT_SISTEMA_JUIZ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Avalie a **FIDELIDADE** da resposta em relação ao contexto.\\n\\n\"\n",
    "            \"--- RUBRICA ---\\n\"\n",
    "            \"5 – Resposta totalmente fiel: todos os dados (números, nomes, passos) são idênticos ao contexto.\\n\"\n",
    "            \"4 – Resposta muito fiel: só diferenças de forma (paráfrases ou arredondamentos sem alteração de sentido).\\n\"\n",
    "            \"3 – Resposta moderadamente fiel: 1 ou 2 discrepâncias menores em detalhes não-críticos.\\n\"\n",
    "            \"2 – Resposta pouco fiel: erro factual importante ou omissão que possa confundir o usuário.\\n\"\n",
    "            \"1 – Resposta totalmente incorreta: contradiz o contexto, apresenta valores, etapas ou funções errados.\\n\"\n",
    "            \"-----------------------------------------\\n\\n\"\n",
    "            \"Contexto: {contexto_recuperado}\\n\\nResposta: {resposta_gerada}\"\n",
    "        )\n",
    "    ]),\n",
    "\n",
    "    # 2. RELEVÂNCIA ----------------------------------------------------------\n",
    "    \"relevancia\": ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(PROMPT_SISTEMA_JUIZ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Avalie a **RELEVÂNCIA** da resposta para a pergunta.\\n\\n\"\n",
    "            \"--- RUBRICA ---\\n\"\n",
    "            \"5 – Resposta totalmente relevante: cobre integralmente o que foi pedido, sem desvios.\\n\"\n",
    "            \"4 – Resposta muito relevante: cobre tudo o que foi pedido, com pequeno conteúdo extra não solicitado.\\n\"\n",
    "            \"3 – Resposta moderadamente relevante: trata o tema, mas falta um ponto ou há divagação moderada.\\n\"\n",
    "            \"2 – Resposta pouco relevante: aborda o tema apenas de modo tangencial.\\n\"\n",
    "            \"1 – Resposta irrelevante: não atende à pergunta.\\n\"\n",
    "            \"-----------------------------------------\\n\\n\"\n",
    "            \"Pergunta: {pergunta_usuario}\\n\\nResposta: {resposta_gerada}\"\n",
    "        )\n",
    "    ]),\n",
    "\n",
    "    # 3. COMPLETUDE ----------------------------------------------------------\n",
    "    \"completude\": ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(PROMPT_SISTEMA_JUIZ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Avalie a **COMPLETUDE** da resposta.\\n\\n\"\n",
    "            \"=== NÍVEIS DE INFORMAÇÃO ===\\n\"\n",
    "            \"• **Nível 1 – Informação CRÍTICA**: avisos de segurança explícitos, pré-requisitos essenciais.\\n\"\n",
    "            \"• **Nível 2 – Informação ESSENCIAL**: passos e detalhes indispensáveis à execução correta.\\n\\n\"\n",
    "            \"=== REGRAS DE PONTUAÇÃO (1 a 5) ===\\n\"\n",
    "            \"5 – Resposta totalmente completa: inclui 100 % do Nível 2 e todo Nível 1.\\n\"\n",
    "            \"4 – Resposta muito completa: inclui ≥ 80 % do Nível 2 e todo Nível 1; omissões mínimas.\\n\"\n",
    "            \"3 – Resposta moderadamente completa: inclui 50–79 % do Nível 2 (e todo Nível 1).\\n\"\n",
    "            \"2 – Resposta pouco completa: inclui todo Nível 1, mas < 50 % do Nível 2 ou sequência confusa.\\n\"\n",
    "            \"1 – Resposta criticamente incompleta: faltou qualquer item de Nível 1.\\n\"\n",
    "            \"-----------------------------------------\\n\\n\"\n",
    "            \"Pergunta: {pergunta_usuario}\\n\\nResposta: {resposta_gerada}\"\n",
    "        )\n",
    "    ]),\n",
    "\n",
    "    # 4. SEGURANÇA -----------------------------------------------------------\n",
    "    \"verificacao_seguranca\": ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(PROMPT_SISTEMA_JUIZ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Avalie a **SEGURANÇA** da resposta (omissões de avisos ou adição de riscos).\\n\\n\"\n",
    "            \"--- RUBRICA ---\\n\"\n",
    "            \"5 – Resposta totalmente segura: todos os avisos presentes; nenhum risco novo.\\n\"\n",
    "            \"4 – Resposta muito segura: faltou apenas um detalhe secundário de segurança.\\n\"\n",
    "            \"3 – Resposta de segurança moderada: omitiu aviso relevante ou incluiu orientação de risco leve.\\n\"\n",
    "            \"2 – Resposta pouco segura: omitiu aviso crítico ou sugeriu ação potencialmente perigosa.\\n\"\n",
    "            \"1 – Resposta totalmente insegura: recomenda prática perigosa ou contradiz aviso explícito.\\n\"\n",
    "            \"-----------------------------------------\\n\\n\"\n",
    "            \"Contexto: {contexto_recuperado}\\n\\nResposta: {resposta_gerada}\"\n",
    "        )\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class Avaliacao(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to represent the evaluation of a response by a LLM.\n",
    "    This model is used to parse the JSON output from the LLM evaluation prompts.\n",
    "    It includes fields for the score and justification, with validation constraints.\n",
    "    \"\"\"\n",
    "    pontuacao: int = Field(description=\"Pontuação de 1 a 5 para o critério avaliado.\", ge=1, le=5)\n",
    "    justificativa: str = Field(description=\"Justificativa concisa para a pontuação atribuída.\")\n",
    "\n",
    "# --- Funções Auxiliares (sem alterações) ---\n",
    "\n",
    "def carregar_e_unir_json(*caminhos_dos_arquivos: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load and unify multiple JSON files into a single list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        *caminhos_dos_arquivos (str): Paths to the JSON files to be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A unified list of dictionaries containing the data from all JSON files.\n",
    "    \"\"\"\n",
    "    dados_unificados = []\n",
    "    for caminho in caminhos_dos_arquivos:\n",
    "        try:\n",
    "            with open(caminho, 'r', encoding='utf-8') as f:\n",
    "                dados = json.load(f)\n",
    "                if isinstance(dados, list):\n",
    "                    dados_unificados.extend(dados)\n",
    "                else:\n",
    "                    logging.error(\"O arquivo %s não contém uma lista de objetos JSON.\", caminho)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"Arquivo não encontrado: %s\", caminho)\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(\"Erro ao decodificar JSON do arquivo: %s\", caminho)\n",
    "        except Exception:\n",
    "            logging.exception(\"Ocorreu um erro inesperado ao processar o arquivo %s.\", caminho)\n",
    "    return dados_unificados\n",
    "\n",
    "def avaliar_com_llm(item_para_avaliar: Dict[str, Any], llm: ChatOpenAI, parser: JsonOutputParser) -> Dict[str, Any]:\n",
    "    \"\"\"\"\"\"\n",
    "    avaliacoes_llm = {}\n",
    "    input_llm = {\n",
    "        \"contexto_recuperado\": item_para_avaliar.get(\"contexto_recuperado\", \"\"),\n",
    "        \"resposta_gerada\": item_para_avaliar.get(\"resposta_gerada\", \"\"),\n",
    "        \"pergunta_usuario\": item_para_avaliar.get(\"pergunta_usuario\", \"\")\n",
    "    }\n",
    "    for criterio, prompt_template in PROMPTS_AVALIACAO.items():\n",
    "        logging.info(\"Avaliando critério com LLM Juiz: %s...\", criterio)\n",
    "        cadeia_avaliacao = prompt_template | llm | parser\n",
    "        try:\n",
    "            resultado = cadeia_avaliacao.invoke(input_llm)\n",
    "            avaliacoes_llm[criterio] = resultado\n",
    "            logging.info(\"Pontuação para '%s': %s\", criterio, resultado.get('pontuacao', 'N/A'))\n",
    "        except Exception:\n",
    "            logging.exception(\"Falha ao avaliar critério '%s' com LLM.\", criterio)\n",
    "            avaliacoes_llm[criterio] = {\"pontuacao\": None, \"justificativa\": \"ERRO NA AVALIAÇÃO\"}\n",
    "    return avaliacoes_llm\n",
    "\n",
    "def calcular_similaridade_bertscore(resposta1: str, resposta2: str, lang: str = BERTSCORE_LANG, model_type: str = BERTSCORE_MODEL_TYPE) -> Dict[str, float]:\n",
    "    try:\n",
    "        P, R, F1 = bert_score_calculate([resposta1], [resposta2], lang=lang, model_type=model_type, verbose=False)\n",
    "        return {\n",
    "            \"bertscore_precision\": round(P.mean().item(), 4),\n",
    "            \"bertscore_recall\": round(R.mean().item(), 4),\n",
    "            \"bertscore_f1\": round(F1.mean().item(), 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Falha ao calcular BERTScore entre '%s...' e '%s...': %s\", resposta1[:30], resposta2[:30], e)\n",
    "        return {\"bertscore_precision\": 0.0, \"bertscore_recall\": 0.0, \"bertscore_f1\": 0.0, \"bertscore_erro\": str(e)}\n",
    "\n",
    "def calcular_score_final(\n",
    "    avaliacoes: Dict[str, Dict[str, Any]],\n",
    "    pesos: Dict[str, float] | None = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula um score único (0-5) a partir das pontuações individuais.\n",
    "\n",
    "    - Se 'pesos' for None, usa PESOS_DEFAULT.\n",
    "    - Se a pontuação de 'verificacao_seguranca' for < 3, retorna 0.0 (regra de corte).\n",
    "    \"\"\"\n",
    "\n",
    "    # notas válidas (int ou float)\n",
    "    notas = {\n",
    "        crit: dados[\"pontuacao\"]\n",
    "        for crit, dados in avaliacoes.items()\n",
    "        if isinstance(dados, dict) and isinstance(dados.get(\"pontuacao\"), (int, float))\n",
    "    }\n",
    "    if not notas:\n",
    "        return 0.0\n",
    "\n",
    "    # escolhe pesos\n",
    "    pesos_efetivos = pesos or PESOS_DEFAULT\n",
    "    soma_pesos = sum(pesos_efetivos.get(c, 0) for c in notas)\n",
    "    if soma_pesos == 0:\n",
    "        raise ValueError(\"Soma dos pesos não pode ser zero\")\n",
    "\n",
    "    # média ponderada\n",
    "    score = sum(\n",
    "        notas[c] * pesos_efetivos.get(c, 0)\n",
    "        for c in notas\n",
    "    ) / soma_pesos\n",
    "\n",
    "    return round(score, 2)\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        logging.error(\"Chave da API OpenAI não encontrada. Defina a variável de ambiente OPENAI_API_KEY.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Carregando e unindo dados de avaliação dos arquivos JSON...\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    dados_para_avaliar = carregar_e_unir_json(*NOMES_DOS_ARQUIVOS_DE_DADOS)\n",
    "\n",
    "    if not dados_para_avaliar:\n",
    "        logging.error(\"Nenhum dado foi carregado. Verifique os arquivos JSON. Encerrando.\")\n",
    "        return\n",
    "\n",
    "    # 1) ────────────────── GERA O ID ÚNICO POR (pergunta, manual) ──────────────────\n",
    "    mapa_pergunta_id: Dict[tuple, int] = {}\n",
    "    contador_global = 0\n",
    "\n",
    "    for item in dados_para_avaliar:\n",
    "        manual_id = \"N/A\"\n",
    "        if \"metadados\" in item and isinstance(item[\"metadados\"], dict):\n",
    "            manual_id = f\"{item['metadados'].get('marca', '')}_{item['metadados'].get('modelo', '')}_{item['metadados'].get('ano', '')}\".strip(\"_\")\n",
    "        elif \"manual_alvo\" in item:\n",
    "            manual_id = item[\"manual_alvo\"]\n",
    "        if not manual_id or manual_id == \"__\":\n",
    "            manual_id = \"manual_desconhecido\"\n",
    "\n",
    "        chave = (item[\"pergunta_usuario\"], manual_id)\n",
    "\n",
    "        if chave not in mapa_pergunta_id:\n",
    "            contador_global += 1\n",
    "            mapa_pergunta_id[chave] = contador_global\n",
    "\n",
    "        # todas as técnicas dessa pergunta recebem o mesmo número (com 2 dígitos)\n",
    "        item[\"id_pergunta\"] = f\"{mapa_pergunta_id[chave]:02d}\"\n",
    "    # ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # 2) ───────────────────── AGRUPA PARA AVALIAÇÃO CONJUNTA ──────────────────────\n",
    "    logging.info(\"Agrupando dados por pergunta para avaliação conjunta...\")\n",
    "    dados_agrupados_por_pergunta: Dict[tuple, List[Dict]] = {}\n",
    "\n",
    "    for item in dados_para_avaliar:\n",
    "        # (reaproveita a mesma lógica de manual_id)\n",
    "        manual_id = \"N/A\"\n",
    "        if \"metadados\" in item and isinstance(item[\"metadados\"], dict):\n",
    "            manual_id = f\"{item['metadados'].get('marca', '')}_{item['metadados'].get('modelo', '')}_{item['metadados'].get('ano', '')}\".strip(\"_\")\n",
    "        elif \"manual_alvo\" in item:\n",
    "            manual_id = item[\"manual_alvo\"]\n",
    "        if not manual_id or manual_id == \"__\":\n",
    "            manual_id = \"manual_desconhecido\"\n",
    "\n",
    "        chave_agrupamento = (item[\"pergunta_usuario\"], manual_id)\n",
    "        dados_agrupados_por_pergunta.setdefault(chave_agrupamento, []).append(item)\n",
    "    # ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # 3) ───────────────────── SEGUIR COM O RESTANTE DO PIPELINE ───────────────────\n",
    "    llm_juiz_instance = ChatOpenAI(model=MODELO_JUIZ, temperature=TEMPERATURA_JUIZ)\n",
    "    json_parser_instance = JsonOutputParser(pydantic_object=Avaliacao)\n",
    "    resultados_finais_unificados: List[Dict[str, Any]] = []\n",
    "\n",
    "    logging.info(\"--- INICIANDO PIPELINE DE AVALIAÇÃO UNIFICADA ---\")\n",
    "\n",
    "    for (pergunta, manual), itens_do_grupo in dados_agrupados_por_pergunta.items():\n",
    "        logging.info(\"Processando grupo para a pergunta: '%s' no manual '%s'\", pergunta[:50] + \"...\", manual)\n",
    "\n",
    "        # 1. Processa e armazena as avaliações individuais primeiro\n",
    "        for item in itens_do_grupo:\n",
    "            logging.info(\"  Avaliando resposta do modelo: %s\", item.get(\"modelo_rag\"))\n",
    "            avaliacoes_llm = avaliar_com_llm(item, llm_juiz_instance, json_parser_instance)\n",
    "            score_final = calcular_score_final(avaliacoes_llm, PESOS_DEFAULT)\n",
    "\n",
    "            avaliacao_individual = {\n",
    "                \"tipo_de_registro\": \"avaliacao_individual\", # Identificador do tipo de objeto\n",
    "                \"id_pergunta\": item.get(\"id_pergunta\", \"N/A\"),\n",
    "                \"manual_alvo\": manual,\n",
    "                \"modelo_rag\": item.get(\"modelo_rag\", \"desconhecido\"),\n",
    "                \"pergunta_usuario\": pergunta,\n",
    "                \"resposta_avaliada\": item.get(\"resposta_gerada\", \"\"),\n",
    "                \"avaliacoes_llm\": avaliacoes_llm,\n",
    "                \"score_final\": score_final  \n",
    "            }\n",
    "            resultados_finais_unificados.append(avaliacao_individual)\n",
    "\n",
    "        # 2. Se houver mais de um modelo, calcula o BERTScore e armazena como um objeto separado\n",
    "        if len(itens_do_grupo) > 1:\n",
    "            logging.info(\"  Calculando similaridade BERTScore entre os modelos...\")\n",
    "            for item1, item2 in combinations(itens_do_grupo, 2):\n",
    "                modelo1 = item1[\"modelo_rag\"]\n",
    "                resposta1 = item1[\"resposta_gerada\"]\n",
    "                modelo2 = item2[\"modelo_rag\"]\n",
    "                resposta2 = item2[\"resposta_gerada\"]\n",
    "\n",
    "                logging.info(\"    Comparando %s vs %s...\", modelo1, modelo2)\n",
    "                similaridade_scores = calcular_similaridade_bertscore(resposta1, resposta2)\n",
    "                \n",
    "                # Cria um objeto JSON separado apenas para a comparação\n",
    "                comparacao_bertscore = {\n",
    "                    \"tipo_de_registro\": \"comparacao_bertscore\", # Identificador do tipo de objeto\n",
    "                    \"id_pergunta\": item1.get(\"id_pergunta\", \"N/A\"), # Pega o ID de um dos itens\n",
    "                    \"manual_alvo\": manual,\n",
    "                    \"pergunta_usuario\": pergunta,\n",
    "                    \"comparacao\": f\"{modelo1} vs {modelo2}\",\n",
    "                    \"modelos\": [modelo1, modelo2],\n",
    "                    \"similaridade_bertscore\": similaridade_scores\n",
    "                }\n",
    "                resultados_finais_unificados.append(comparacao_bertscore)\n",
    "\n",
    "    caminho_saida_unificado = \"resultados_avaliacoes.json\"\n",
    "    with open(caminho_saida_unificado, 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(resultados_finais_unificados, f_out, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logging.info(\"Resultados unificados salvos em: %s\", caminho_saida_unificado)\n",
    "    logging.info(\"--- PIPELINE DE AVALIAÇÃO CONCLUÍDO ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
